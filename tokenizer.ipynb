{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{('ak', 'u', '</w>'): 0, ('s', 'u', 'k', 'a</w>'): 1, ('me', 'makan</w>'): 2, ('nasi</w>',): 3, ('di', 'makan</w>'): 4, ('a', 'y', 'a', 'm', '</w>'): 5, ('makan</w>',): 6, ('di', 'me', 'j', 'a</w>'): 7}\n"
     ]
    }
   ],
   "source": [
    "corpus = [[\"aku\", \"suka\", \"memakan\", \"nasi\"], [\"nasi\", \"dimakan\", \"ayam\"], [\"makan\", \"dimeja\" ,\"makan\"]]\n",
    "def tokenizer(corpus):\n",
    "    vocab = {}\n",
    "    for kalimat in corpus:\n",
    "        for kata in kalimat:\n",
    "            char = list(kata) + [\"</w>\"]\n",
    "            char = tuple(char)\n",
    "            if char in vocab:\n",
    "                vocab[char] += 1\n",
    "            else:\n",
    "                vocab[char] = 1\n",
    "    while True:\n",
    "        pair_freq = {}\n",
    "        for token, freq in vocab.items():\n",
    "            for i in range(len(token) - 1):\n",
    "                pair = (token[i], token[i+1])\n",
    "                if pair in pair_freq:\n",
    "                    pair_freq[pair] += freq\n",
    "                else:\n",
    "                    pair_freq[pair] = freq\n",
    "        best_value = max(pair_freq.values())\n",
    "        best_pair = max(pair_freq, key=pair_freq.get)\n",
    "\n",
    "        if best_value < 2:\n",
    "            break\n",
    "\n",
    "        new_vocab = {}\n",
    "        for token, freq in vocab.items():\n",
    "            new_token = []\n",
    "            i = 0\n",
    "            while i < len(token):\n",
    "                if i < len(token) - 1 and (token[i], token[i+1]) == best_pair:\n",
    "                    new_token.append(token[i] + token[i+1])\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_token.append(token[i])\n",
    "                    i += 1\n",
    "            new_vocab[tuple(new_token)] = freq\n",
    "\n",
    "        vocab = new_vocab\n",
    "        \n",
    "    return vocab\n",
    "\n",
    "    \n",
    "vocab = tokenizer(corpus)\n",
    "vocab_index = {}\n",
    "for i, token in enumerate(vocab.keys()):\n",
    "    vocab_index[token] = i\n",
    "\n",
    "print(vocab_index)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " vocab kita di atas tadi masi voca per kata, bukan karakter unik. misalnya dimakann dan memakan, makannnya masih ada dua, yang kita perlu kan makan tu satu aja. jadi kita perlu ubah vocab kita ga perkata lagi dan kalau ada sub kata yang sama dalam satu kalimat kita gabungin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'</w>': 0, 'a': 1, 'a</w>': 2, 'ak': 3, 'di': 4, 'j': 5, 'k': 6, 'm': 7, 'makan</w>': 8, 'me': 9, 'nasi</w>': 10, 's': 11, 'u': 12, 'y': 13}\n"
     ]
    }
   ],
   "source": [
    "token_set = set() # buat set kosong untuk menyimpan token. set() adalah fungsi untuk membuang yang sama\n",
    "for i in vocab.keys():# loop untuk setiap token dalam vocab\n",
    "    for j in i: # loop untuk setiap is9 dari tuple  token\n",
    "        token_set.add(j) # tambahkan token ke token set dan hapus yang sama\n",
    "token_set = sorted(token_set) # urutkan token set sesuai aplhabet menggubakan sorted(ini bentuk yang biasanya dipake nlp)\n",
    "vocab_dict = {i: idx for idx,i in enumerate(token_set) }# ambil setiap isi dari token set dan buat dictionary dengan key dan value nya indexnya sendiri\n",
    "print(vocab_dict) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'[UNK]'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 27\u001b[0m\n\u001b[0;32m     25\u001b[0m         encoded\u001b[38;5;241m.\u001b[39mappend(encoded_kalimat)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m encoded\n\u001b[1;32m---> 27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mencode_corpus\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_dict\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[1;32mIn[24], line 23\u001b[0m, in \u001b[0;36mencode_corpus\u001b[1;34m(corpus, vocab_dict)\u001b[0m\n\u001b[0;32m     21\u001b[0m encoded_kalimat \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m kata \u001b[38;5;129;01min\u001b[39;00m kalimat:\n\u001b[1;32m---> 23\u001b[0m     encoded_kata \u001b[38;5;241m=\u001b[39m \u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m     encoded_kalimat\u001b[38;5;241m.\u001b[39mextend(encoded_kata)\n\u001b[0;32m     25\u001b[0m encoded\u001b[38;5;241m.\u001b[39mappend(encoded_kalimat)\n",
      "Cell \u001b[1;32mIn[30], line 15\u001b[0m, in \u001b[0;36mencode\u001b[1;34m(word, vocab_dict)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m merged:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[43mvocab_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[UNK]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_corpus\u001b[39m(corpus, vocab_dict):\n",
      "Cell \u001b[1;32mIn[30], line 15\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m merged:\n\u001b[0;32m     13\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m indices \u001b[38;5;241m=\u001b[39m [vocab_dict\u001b[38;5;241m.\u001b[39mget(t, \u001b[43mvocab_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m[UNK]\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m) \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m tokens]\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indices\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mencode_corpus\u001b[39m(corpus, vocab_dict):\n",
      "\u001b[1;31mKeyError\u001b[0m: '[UNK]'"
     ]
    }
   ],
   "source": [
    "def encode(word, vocab_dict):\n",
    "    tokens = list(word) + [\"</w>\"]\n",
    "    \n",
    "    while True:\n",
    "        merged = False\n",
    "        for i in range(len(tokens)-1):\n",
    "            pair = tokens[i] + tokens[i+1]\n",
    "            if pair in vocab_dict:\n",
    "                tokens = tokens[:i] + [pair] + tokens[i+2:]\n",
    "                merged = True\n",
    "                break\n",
    "        if not merged:\n",
    "            break\n",
    "    \n",
    "    indices = [vocab_dict.get(t, vocab_dict[\"[UNK]\"]) for t in tokens]\n",
    "    return indices\n",
    "\n",
    "    def encode_corpus(corpus, vocab_dict):\n",
    "     encoded = []\n",
    "     for kalimat in corpus:\n",
    "        encoded_kalimat = []\n",
    "        for kata in kalimat:\n",
    "            encoded_kata = encode(kata, vocab_dict)\n",
    "            encoded_kalimat.extend(encoded_kata)\n",
    "        encoded.append(encoded_kalimat)\n",
    "    return encoded\n",
    "print(encode_corpus(corpus, vocab_dict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
